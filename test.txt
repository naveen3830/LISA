import streamlit as st
import pandas as pd
from st_aggrid import AgGrid
from langchain_groq import ChatGroq
from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate
from dotenv import load_dotenv
import os
from io import BytesIO
from markdown import markdown
from weasyprint import HTML, CSS

# Load environment variables
load_dotenv()

# Set Streamlit page configuration
st.set_page_config(page_title="LISA: LLM Informed Statistical Analysis", page_icon=":books:", layout="wide")
tab1, tab2, tab3 = st.tabs(["Home", "ChatBot", "Data Cleaning"])

# Sidebar configuration
with st.sidebar:
    with st.sidebar.expander(":Red[Get Your API Key Here]"):
        st.markdown("""
            ## How to use
            1. Enter your [Groq API key](https://console.groq.com/keys) belowðŸ”‘
            2. Upload a CSV fileðŸ“„
            3. Let LISA do its work!!!ðŸ’¬
        """)
    
    groq_api_key = st.text_input(
        "Enter your Groq API key:", type="password",
        placeholder="Paste your Groq API key here (gsk_...)",
        help="You can get your API key from https://console.groq.com/keys"
    )
    
    st.text("The below parameters like temperature and top-p play a crucial role in controlling the randomness and creativity of the generated text. Adjust these parameters according to your requirements.")
    with st.sidebar.expander("Model Parameters"):
        model_name = st.selectbox("Select Model:", ["llama3-8b-8192", "llama3-70b-8192", "mixtral-8x7b-32768", "gemma-7b-it", "gemma2-9b-it"])
        temperature = st.slider("Temperature:", min_value=0.0, max_value=1.0, value=0.5, step=0.1)
        top_p = st.slider("Top-p:", min_value=0.0, max_value=1.0, value=1.0, step=0.25)

    st.divider()

    llm = None
    if groq_api_key:
        try:
            llm = ChatGroq(
                groq_api_key=groq_api_key,
                model_name=model_name,
                temperature=temperature,
                top_p=top_p
            )
        except Exception as e:
            st.sidebar.error(f"Error initializing model: {str(e)}")

def create_chat_prompt(df_info):
    system_message = SystemMessagePromptTemplate.from_template("You are StatBot, an expert statistical analyst. Explain the output in simple English.")
    human_message = HumanMessagePromptTemplate.from_template('The columns in the dataset are: {columns}')
    chat_prompt = ChatPromptTemplate.from_messages([system_message, human_message])
    return chat_prompt.format_messages(columns=df_info)

def create_pdf_file(content: str) -> BytesIO:
    """
    Create a PDF file from the provided Markdown content.
    Converts Markdown to styled HTML, then HTML to PDF.
    """
    html_content = markdown(content, extensions=["extra", "codehilite"])

    styled_html = f"""
    <html>
        <head>
            <style>
                @page {{
                    margin: 2cm;
                }}
                body {{
                    font-family: Arial, sans-serif;
                    line-height: 1.6;
                    font-size: 12pt;
                }}
                h1, h2, h3, h4, h5, h6 {{
                    color: #333366;
                    margin-top: 1em;
                    margin-bottom: 0.5em;
                }}
                p {{
                    margin-bottom: 0.5em;
                }}
                code {{
                    background-color: #f4f4f4;
                    padding: 2px 4px;
                    border-radius: 4px;
                    font-family: monospace;
                    font-size: 0.9em;
                }}
                pre {{
                    background-color: #f4f4f4;
                    padding: 1em;
                    border-radius: 4px;
                    white-space: pre-wrap;
                    word-wrap: break-word;
                }}
                blockquote {{
                    border-left: 4px solid #ccc;
                    padding-left: 1em;
                    margin-left: 0;
                    font-style: italic;
                }}
                table {{
                    border-collapse: collapse;
                    width: 100%;
                    margin-bottom: 1em;
                }}
                th, td {{
                    border: 1px solid #ddd;
                    padding: 8px;
                    text-align: left;
                }}
                th {{
                    background-color: #f2f2f2;
                }}
                input, textarea {{
                    border-color: #4A90E2 !important;
                }}
            </style>
        </head>
        <body>
            {html_content}
        </body>
    </html>
    """

    pdf_buffer = BytesIO()
    HTML(string=styled_html).write_pdf(pdf_buffer)
    pdf_buffer.seek(0)

    return pdf_buffer

with tab1:
    st.header("Welcome to LISA: LLM Informed Statistical Analysis ðŸŽˆ")
    st.markdown("""
        LISA is an innovative platform designed to automate your data analysis process using advanced Large Language Models (LLM) for insightful inferences. Whether you're a data enthusiast, researcher, or business analyst, LISA simplifies complex data tasks, providing clear and comprehensible explanations for your data.
        
        LISA combines the efficiency of automated data processing with the intelligence of modern language models to deliver a seamless and insightful data analysis experience. Empower your data with LISA!
    """)
    st.divider()

    uploaded_file = st.file_uploader("Upload a CSV file", type=['csv'])

    all_responses = []

    if uploaded_file is not None:
        df = pd.read_csv(uploaded_file)
        AgGrid(df, theme="balham")
        st.divider()

        option = st.selectbox("Select an option:", ["Show dataset dimensions", "Display data description", "Verify data integrity", "Summarize numerical data statistics", "Summarize categorical data"])

        if not groq_api_key:
            st.warning("Please enter your Groq API key in the sidebar to use the analysis features.")
        elif llm is None:
            st.error("Failed to initialize the model. Please check your API key.")
        else:
            if option == "Show dataset dimensions":
                shape_of_the_data = df.shape
                response = llm.invoke(create_chat_prompt(shape_of_the_data))
                st.write(response.content)
                all_responses.append(f"## Show dataset dimensions\n\n{response.content}")

            elif option == "Display data description":
                column_description = df.columns
                response = llm.invoke(create_chat_prompt(column_description))
                st.write(response.content)
                all_responses.append(f"## Display data description\n\n{response.content}")

            elif option == "Verify data integrity":
                df_check = check(df)
                st.dataframe(df_check)
                response = llm.invoke(create_chat_prompt(df_check))
                st.write(response.content)
                all_responses.append(f"## Verify data integrity\n\n{response.content}")

            elif option == "Summarize numerical data statistics":
                describe_numerical = df.describe().T
                st.dataframe(describe_numerical)
                response = llm.invoke(create_chat_prompt(describe_numerical))
                st.write(response.content)
                all_responses.append(f"## Summarize numerical data statistics\n\n{response.content}")

            elif option == "Summarize categorical data":
                categorical_df = df.select_dtypes(include=['object'])
                if categorical_df.empty:
                    st.write("No categorical columns found.")
                else:
                    describe_categorical = categorical_df.describe()
                    st.dataframe(describe_categorical)
                    response = llm.invoke(create_chat_prompt(describe_categorical))
                    st.write(response.content)
                    all_responses.append(f"## Summarize categorical data\n\n{response.content}")

    # Button to download the generated content
    if st.button("Download all responses as PDF"):
        combined_content = "\n\n".join(all_responses)
        pdf_file = create_pdf_file(combined_content)
        st.download_button(
            label="Download PDF",
            data=pdf_file,
            file_name='generated_content.pdf',
            mime='application/pdf'
        )


with tab2:
    st.markdown("""Our integrated chatbot is available to assist you, providing real-time answers to your data-related queries and enhancing your overall experience with personalized support.""")
    st.markdown("""---""")

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    def get_response(query, chat_history):
        template = """
        You are a helpful assistant. Answer the following the user asks:
        
        Chat history:{chat_history}
        user question:{user_question}
        """
        prompt = ChatPromptTemplate.from_template(template)
        chain = prompt | llm | StrOutputParser()
        return chain.stream({
            "chat_history": chat_history,"user_question": query})

    for message in st.session_state.chat_history:
        if isinstance(message, HumanMessage):
            with st.chat_message("Human"):
                st.markdown(message.content)
        else:
            with st.chat_message("AI"):
                st.markdown(message.content)
                
    if not groq_api_key:
        st.warning("Please enter your Groq API key in the sidebar to use the chatbot.")
    elif llm is None:
        st.error("Failed to initialize the model. Please check your API key.")
    else:
        user_query = st.chat_input("Type your message here")
        
        if user_query is not None and user_query != "":
            st.session_state.chat_history.append(HumanMessage(user_query))
            
            with st.chat_message("Human"):
                st.markdown(user_query)
                
            with st.chat_message("AI"):
                ai_response = st.write_stream(get_response(user_query, st.session_state.chat_history))
                
            st.session_state.chat_history.append(AIMessage(ai_response))

# with tab3:
#     interactive_data_cleaning()
    